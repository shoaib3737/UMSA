{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMy7RbWZNiO5Zw/IY4S8ll"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1. Mounting Google Drive"],"metadata":{"id":"QWugslnrFzCF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tm8kphdkFs3e"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#2. Installing Packages"],"metadata":{"id":"l2Z8Z-rnF4Qr"}},{"cell_type":"code","source":["!pip install SpeechRecognition\n","!pip install google-cloud-speech pandas\n","!pip install pydub\n","!apt install ffmpeg\n","!pip install FER"],"metadata":{"id":"bbiT8mZmF2Cr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. Dataset Exploration\n","\n","\n","\n","*   Importing Libraries\n","*   List item\n","\n"],"metadata":{"id":"UWF6TWjpF8az"}},{"cell_type":"code","source":["import os\n","import speech_recognition as sr\n","import spacy\n","import pandas as pd\n","from collections import Counter\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import re\n","from google.cloud import speech\n","import pandas as pd\n","import os\n","import glob\n","from pydub import AudioSegment\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","from fer import FER"],"metadata":{"id":"v3hzsBeMF9CD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Labled Transcriptions\n","\n","\n","\n","*   **updated_transcriptions_with_senti.csv**\n","*   Senti coloum 0 values unique\n","\n"],"metadata":{"id":"974bhgnOIRuZ"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/work2/updated_transcriptions_with_senti.csv')\n","df.head(111)"],"metadata":{"id":"7xce6qZzISUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[df['Senti'] == 0].head(111)\n"],"metadata":{"id":"65CLuc-pIV_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = df[df['Senti'] == 0].shape[0]\n","print(count)"],"metadata":{"id":"fV0AYvvsIZGE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Saved audio analysis in **transcription_results_final.csv**"],"metadata":{"id":"hHoRclpVIbN0"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import speech_recognition as sr\n","from pydub import AudioSegment\n","\n","def get_audio_length(file_path):\n","    \"\"\"Return the length of the audio file in seconds.\"\"\"\n","    audio = AudioSegment.from_file(file_path)\n","    return len(audio) / 1000.0  # Convert milliseconds to seconds\n","\n","def transcribe_audio_speech_recognition(audio_file_path):\n","    \"\"\"Transcribe the given local audio file using the speech_recognition library.\"\"\"\n","    recognizer = sr.Recognizer()\n","    with sr.AudioFile(audio_file_path) as source:\n","        audio_data = recognizer.record(source)\n","        try:\n","            return recognizer.recognize_google(audio_data, language=\"ur-PK\")\n","        except sr.UnknownValueError:\n","            print(\"Google Speech Recognition could not understand the audio\")\n","            return None\n","        except sr.RequestError as e:\n","            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n","            return None\n","\n","def read_words_from_file(file_path):\n","    \"\"\"Read words from a file and return them as a list.\"\"\"\n","    try:\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            return [line.strip().lower() for line in file.readlines()]\n","    except FileNotFoundError:\n","        print(f\"File not found: {file_path}\")\n","        return []\n","    except Exception as e:\n","        print(f\"Error reading file {file_path}: {e}\")\n","        return []\n","\n","def analyze_sentiment(words_in_transcript, negative_words, positive_words):\n","    negative_count, positive_count = 0, 0\n","    negative_positions, positive_positions = [], []\n","    for index, word in enumerate(words_in_transcript):\n","        position = (index + 1) / len(words_in_transcript) * length_in_seconds\n","        if word.lower() in negative_words:\n","            negative_count += 1\n","            negative_positions.append(round(position))\n","        elif word.lower() in positive_words:\n","            positive_count += 1\n","            positive_positions.append(round(position))\n","    return negative_count, positive_count, negative_positions, positive_positions\n","\n","# Path to the folder containing audio files\n","audio_folder_path = '/content/drive/MyDrive/work2/wav'\n","negative_words = read_words_from_file('/content/drive/MyDrive/work2/dictionary/urdu_negative_words.txt')\n","positive_words = read_words_from_file('/content/drive/MyDrive/work2/dictionary/urdu_positive_words.txt')\n","\n","results = []\n","for audio_file in os.listdir(audio_folder_path):\n","    if audio_file.endswith('.wav'):\n","        audio_file_path = os.path.join(audio_folder_path, audio_file)\n","        length_in_seconds = get_audio_length(audio_file_path)\n","        transcript = transcribe_audio_speech_recognition(audio_file_path)\n","\n","        if transcript:\n","            words_in_transcript = transcript.split()\n","            negative_count, positive_count, negative_positions, positive_positions = analyze_sentiment(words_in_transcript, negative_words, positive_words)\n","\n","            results.append({\n","                'Length': length_in_seconds,\n","                'audio_name': audio_file,\n","                'Transcript': transcript,\n","                'negative_count': negative_count,\n","                'positive_count': positive_count,\n","                'negative_positions': negative_positions,\n","                'positive_positions': positive_positions\n","            })\n","\n","# Save results to a CSV file\n","csv_file_path = '/content/drive/MyDrive/work2/Final/transcription_results_final.csv'\n","df = pd.DataFrame(results)\n","df.to_csv(csv_file_path, index=False)\n","print(f\"Results saved to {csv_file_path}\")\n"],"metadata":{"id":"-nikPimwIbr8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Show transcription_results_final.csv"],"metadata":{"id":"-3hB9__nIjBc"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/work2/Final/transcription_results_final.csv')\n","df.head(12)"],"metadata":{"id":"o9AG5XpDIlp0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/work2/Final/transcription_results_final.csv')\n","df.info()"],"metadata":{"id":"CmNdDbwkIor1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/work2/Final/transcription_results_final.csv')\n","df.head()"],"metadata":{"id":"fHpRvulyIrrt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extraction of all FRAMES"],"metadata":{"id":"xfUbGOToGLFM"}},{"cell_type":"code","source":["import cv2\n","import pandas as pd\n","import os\n","\n","# Function to extract frames\n","def extract_frames(video_file_path, output_folder):\n","    video_name = os.path.basename(video_file_path).split('.')[0]\n","    cap = cv2.VideoCapture(video_file_path)\n","    frame_info = []\n","    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    duration = int(total_frames / frame_rate)\n","\n","    for current_time in range(duration + 1):\n","        cap.set(cv2.CAP_PROP_POS_MSEC, current_time * 1000)\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame_filename = os.path.join(output_folder, f'{video_name}_frame_{current_time}.jpg')\n","        cv2.imwrite(frame_filename, frame)\n","        frame_info.append({'Video Name': video_name, 'Frame Path': frame_filename, 'Time in Seconds': current_time})\n","\n","    cap.release()\n","    return frame_info\n","\n","# Directory containing videos\n","video_directory = '/content/drive/MyDrive/work2/videos'\n","\n","output_base_folder_Training = '/content/drive/MyDrive/work2/Training_Data/train_frames/All_Frames'\n","output_base_folder_Testing = '/content/drive/MyDrive/work2/Testing_Data/test_frames/All_Frames'\n","\n","all_frame_info = []  # Store info for all frames from all videos\n","\n","# Process each video file\n","video_files = sorted(os.listdir(video_directory))\n","for i, video_file in enumerate(video_files):\n","    video_file_path = os.path.join(video_directory, video_file)\n","    video_name = os.path.splitext(video_file)[0]\n","\n","    # Skip files that are not named with numbers (e.g., 1.mp4, 2.mp4, etc.)\n","    if not video_name.isdigit():\n","        continue\n","\n","    video_num = int(video_name)\n","\n","    # Determine output folder based on video file number\n","    if 1 <= video_num <= 77:\n","        output_folder = os.path.join(output_base_folder_Training, video_name)\n","    elif 78 <= video_num <= 111:\n","        output_folder = os.path.join(output_base_folder_Testing, video_name)\n","    else:\n","        continue\n","\n","    # Create output folder for this video\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    # Extract frames\n","    frame_info = extract_frames(video_file_path, output_folder)\n","    all_frame_info.extend(frame_info)\n","\n","# Save frame information to a new CSV file\n","frames_csv_path = '/content/drive/MyDrive/work2/Final/Extraction_Frames_results_final.csv'\n","frame_info_df = pd.DataFrame(all_frame_info)\n","frame_info_df.to_csv(frames_csv_path, index=False)\n","\n","print(\"Frames extracted and saved.\")\n","print(f\"Frame information saved to {frames_csv_path}\")\n"],"metadata":{"id":"CbA6bp1bGLpM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extraction of Key Frames"],"metadata":{"id":"in1rMV4cGNED"}},{"cell_type":"code","source":["import cv2\n","import pandas as pd\n","import os\n","\n","# Function to extract frames at specified times\n","def extract_frames(video_file_path, times_in_seconds, output_folder):\n","    video_name = os.path.basename(video_file_path).split('.')[0]\n","    cap = cv2.VideoCapture(video_file_path)\n","    frame_info = []\n","    extracted_times = set()\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        current_time = int(cap.get(cv2.CAP_PROP_POS_MSEC) // 1000)\n","        if current_time in times_in_seconds and current_time not in extracted_times:\n","            frame_filename = os.path.join(output_folder, f'{video_name}_frame_{current_time}.jpg')\n","            cv2.imwrite(frame_filename, frame)\n","            frame_info.append({'Video Name': video_name, 'Frame Path': frame_filename, 'Time in Seconds': current_time})\n","            extracted_times.add(current_time)\n","    cap.release()\n","    return frame_info\n","\n","# Directory containing videos\n","video_directory = '/content/drive/MyDrive/work2/videos'\n","\n","output_base_folder_Training = '/content/drive/MyDrive/work2/Training_Data/train_frames/Key_Frames'\n","output_base_folder_Testing = '/content/drive/MyDrive/work2/Testing_Data/test_frames/Key_Frames'\n","\n","# Load positions from the CSV file\n","csv_file_path = '/content/drive/MyDrive/work2/Final/transcription_results_final.csv'\n","df = pd.read_csv(csv_file_path)\n","\n","# Convert positions to lists of integers\n","df['negative_positions'] = df['negative_positions'].apply(lambda x: eval(x))\n","df['positive_positions'] = df['positive_positions'].apply(lambda x: eval(x))\n","\n","all_frame_info = []  # Store info for all frames from all videos\n","\n","# Process each video file\n","video_files = sorted(os.listdir(video_directory))\n","for i, video_file in enumerate(video_files):\n","    video_file_path = os.path.join(video_directory, video_file)\n","    video_name = os.path.splitext(video_file)[0]\n","\n","    # Skip files that are not named with numbers (e.g., 1.mp4, 2.mp4, etc.)\n","    if not video_name.isdigit():\n","        continue\n","\n","    video_num = int(video_name)\n","\n","    # Determine output folder based on video file number\n","    if 1 <= video_num <= 77:\n","        output_folder = os.path.join(output_base_folder_Training, video_name)\n","    elif 78 <= video_num <= 111:\n","        output_folder = os.path.join(output_base_folder_Testing, video_name)\n","    else:\n","        continue\n","\n","    # Create output folder for this video\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    # Filter the dataframe for the current video\n","    video_df = df[df['audio_name'].str.contains(video_name)]\n","\n","    # Concatenate, flatten, and remove duplicates from positions\n","    all_positions = video_df['negative_positions'].tolist() + video_df['positive_positions'].tolist()\n","    times_in_seconds = list(set([int(time) for sublist in all_positions for time in sublist]))\n","\n","    # Extract frames\n","    frame_info = extract_frames(video_file_path, times_in_seconds, output_folder)\n","    all_frame_info.extend(frame_info)\n","\n","# Save frame information to a new CSV file\n","frames_csv_path = '/content/drive/MyDrive/work2/Final/Extraction_Key_Frames_results_final.csv'\n","frame_info_df = pd.DataFrame(all_frame_info)\n","frame_info_df.to_csv(frames_csv_path, index=False)\n","\n","print(\"Frames extracted and saved.\")\n","print(f\"Frame information saved to {frames_csv_path}\")\n"],"metadata":{"id":"fs3B34gHGPY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#FER on All Frames"],"metadata":{"id":"-4BPABRVKM4N"}},{"cell_type":"code","source":["import pandas as pd\n","import cv2\n","from fer import FER\n","\n","# Function to analyze the most prominent face in a frame and categorize emotions\n","def analyze_most_prominent_face(frame_path):\n","    try:\n","        detector = FER(mtcnn=True)\n","        image = cv2.imread(frame_path)\n","        face_detections = detector.detect_emotions(image)\n","\n","        if face_detections:\n","            largest_face_area = 0\n","            positive_sentiment = 0\n","            negative_sentiment = 0\n","            neutral_sentiment = 0\n","\n","            # Find the most prominent face in the frame\n","            for face in face_detections:\n","                x, y, w, h = face[\"box\"]\n","                face_area = w * h\n","                if face_area > largest_face_area:\n","                    largest_face_area = face_area\n","                    emotions = face[\"emotions\"]\n","\n","                    # Categorize emotions into positive and negative sentiments\n","                    if any(emotions.get(emotion, 0) > 0.5 for emotion in ['happy', 'surprise']):\n","                        positive_sentiment = 1\n","                    if any(emotions.get(emotion, 0) > 0.5 for emotion in ['angry', 'fear', 'disgust']):\n","                        negative_sentiment = 2\n","                    if any(emotions.get(emotion, 0) > 0.5 for emotion in ['neutral', 'sad']):\n","                        neutral_sentiment = 0\n","\n","            return positive_sentiment, negative_sentiment,neutral_sentiment\n","        else:\n","            return 9, 9 ,9  # Neutral for no face\n","    except Exception as e:\n","        print(f\"Error in analyzing frame {frame_path}: {e}\")\n","        return 'Error', 'Error'\n","\n","# Load transcription and frame details DataFrames\n","transcription_df = pd.read_csv('/content/drive/MyDrive/work2/Final/transcription_results_final.csv')\n","frame_details_df = pd.read_csv('/content/drive/MyDrive/work2/Final/extracted_frames_info.csv')\n","\n","# Convert positions in transcription data to lists of integers\n","transcription_df['negative_positions'] = transcription_df['negative_positions'].apply(lambda x: eval(x))\n","transcription_df['positive_positions'] = transcription_df['positive_positions'].apply(lambda x: eval(x))\n","\n","# Initialize sentiment columns in frame details DataFrame\n","frame_details_df['Positive Sentiment'] = None\n","frame_details_df['Negative Sentiment'] = None\n","\n","# Analyze frames for sentiment and assign to corresponding positions\n","for index, row in frame_details_df.iterrows():\n","    frame_time = row['Time in Seconds']\n","    frame_path = row['Frame Path']\n","    positive_sentiment, negative_sentiment,neutral_sentiment = analyze_most_prominent_face(frame_path)\n","\n","    if any(frame_time in positions for positions in transcription_df['positive_positions']):\n","        frame_details_df.at[index, 'Positive Sentiment'] = positive_sentiment\n","    if any(frame_time in positions for positions in transcription_df['negative_positions']):\n","        frame_details_df.at[index, 'Negative Sentiment'] = negative_sentiment\n","\n","# Save updated frame details with FER analysis results\n","frame_details_with_sentiment_path = '/content/drive/MyDrive/work2/Final/frame_details_with_sentiment.csv'\n","frame_details_df.to_csv(frame_details_with_sentiment_path, index=False)\n","print(f\"Updated frame details with FER analysis saved to {frame_details_with_sentiment_path}\")\n","\n","def compile_sentiments(row, frame_details):\n","    positive_sentiments = []\n","    negative_sentiments = []\n","\n","    # Use set to ensure unique positions are considered\n","    unique_negative_positions = set(row['negative_positions'])\n","    unique_positive_positions = set(row['positive_positions'])\n","\n","    for pos in unique_negative_positions:\n","        sentiment = frame_details.loc[frame_details['Time in Seconds'] == pos, 'Negative Sentiment'].values\n","        if sentiment.size > 0:\n","            negative_sentiments.append(sentiment[0])  # Append the first sentiment found\n","\n","    for pos in unique_positive_positions:\n","        sentiment = frame_details.loc[frame_details['Time in Seconds'] == pos, 'Positive Sentiment'].values\n","        if sentiment.size > 0:\n","            positive_sentiments.append(sentiment[0])  # Append the first sentiment found\n","\n","    return pd.Series([negative_sentiments, positive_sentiments])\n","\n","# Compile sentiments into transcription DataFrame\n","transcription_df[['Negative_Frame_Sentiments', 'Positive_Frame_Sentiments']] = transcription_df.apply(\n","    lambda row: compile_sentiments(row, frame_details_df), axis=1\n",")\n","\n","# Save updated transcription results with compiled sentiments\n","transcription_results_with_compiled_sentiments_path = '/content/drive/MyDrive/work2/Final/transcription_results_with_compiled_sentiments.csv'\n","transcription_df.to_csv(transcription_results_with_compiled_sentiments_path, index=False)\n","print(f\"Transcription results with compiled sentiments saved to {transcription_results_with_compiled_sentiments_path}\")"],"metadata":{"id":"qXW0BeCfKQLN"},"execution_count":null,"outputs":[]}]}